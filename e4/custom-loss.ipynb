{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Custom Loss Function in PyTorch\n",
    "\n",
    "In this notebook we will write a custom loss function (Huber Loss)\n",
    "\n",
    "\n",
    "The **Huber loss** is a loss function used in regression tasks, which is less sensitive to outliers than the mean squared error (MSE) loss. It combines the properties of both the L2 loss (squared error) and the L1 loss (absolute error). The Huber loss is defined as:\n",
    "\n",
    "$$\n",
    "L_{\\delta}(y, \\hat{y}) = \n",
    "\\begin{cases} \n",
    "\\frac{1}{2}(y - \\hat{y})^2 & \\text{for } |y - \\hat{y}| \\leq \\delta, \\\\\n",
    "\\delta \\cdot (|y - \\hat{y}| - \\frac{1}{2} \\delta) & \\text{for } |y - \\hat{y}| > \\delta,\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $y$ is the true value,\n",
    "- $\\hat{y}$ is the predicted value,\n",
    "- $\\delta$ is a threshold parameter that controls the transition between L1 and L2 loss.\n",
    "\n",
    "Extra Details: https://en.wikipedia.org/wiki/Huber_loss\n",
    "\n",
    "<details>\n",
    "  <summary>ðŸ’¡ Hint</summary>\n",
    "  Some details: https://www.kaggle.com/code/bigironsphere/loss-function-library-keras-pytorch/notebook\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "torch.manual_seed(42)\n",
    "X = torch.rand(100, 1) * 10  # 100 data points between 0 and 10\n",
    "y = 2 * X + 3 + torch.randn(100, 1)  # Linear relationship with noise\n",
    "\n",
    "#TODO: Define the nn.Module for the Huber Loss\n",
    "class HuberLoss(nn.Module):\n",
    "    ...\n",
    "\n",
    "\n",
    "# Define the Linear Regression Model\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(1, 1)  # Single input and single output\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = LinearRegressionModel()\n",
    "#TODO: Add the loss \n",
    "criterion = ...\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    predictions = model(X)\n",
    "    loss = criterion(predictions, y)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Log progress every 100 epochs\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the learned parameters\n",
    "[w, b] = model.linear.parameters()\n",
    "print(f\"Learned weight: {w.item():.4f}, Learned bias: {b.item():.4f}\")\n",
    "\n",
    "# Testing on new data\n",
    "X_test = torch.tensor([[4.0], [7.0]])\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_test)\n",
    "    print(f\"Predictions for {X_test.tolist()}: {predictions.tolist()}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
