{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNtFK3a/QwF30sQnKomWb/D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AshishBora/TorchLeet/blob/main/softmax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xGlsqj2YAMj"
      },
      "outputs": [],
      "source": [
        "# Implement Softmax function from scratch\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def custom_softmax(logits, axis=0):\n",
        "    max_logits = torch.max(logits, axis, keepdims=True)[0]\n",
        "    logits = logits - max_logits\n",
        "    exp_logits = torch.exp(logits)\n",
        "    axis_sum = torch.sum(exp_logits, axis, keepdims=True)\n",
        "    return exp_logits / axis_sum\n",
        "\n",
        "\n",
        "# TODO: Make this a parameterized test\n",
        "def test_softmax():\n",
        "    # test 1\n",
        "    logits = torch.Tensor([0, 0, 0, 0])\n",
        "    expected_softmax = [0.25 for _ in range(4)]\n",
        "    softmax = custom_softmax(logits)\n",
        "    np.testing.assert_allclose(softmax, expected_softmax)\n",
        "\n",
        "    # test 2\n",
        "    logits = torch.Tensor([1, 1, 1, 1])\n",
        "    expected_softmax = [0.25 for _ in range(4)]\n",
        "    softmax = custom_softmax(logits)\n",
        "    np.testing.assert_allclose(softmax, expected_softmax)\n",
        "\n",
        "    # test 3\n",
        "    logits = torch.Tensor([-np.inf, -np.inf, 1, 1])\n",
        "    expected_softmax = [0, 0, 0.5, 0.5]\n",
        "    softmax = custom_softmax(logits)\n",
        "    np.testing.assert_allclose(softmax, expected_softmax)\n",
        "\n",
        "    # test 4\n",
        "    logits = torch.Tensor([0, 1, 2, 3])\n",
        "    expected_softmax = [0.0320586 , 0.08714432, 0.23688282, 0.64391426]\n",
        "    softmax = custom_softmax(logits)\n",
        "    np.testing.assert_allclose(softmax, expected_softmax, atol=1e-4)\n",
        "\n",
        "\n",
        "test_softmax()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install parameterized"
      ],
      "metadata": {
        "id": "IgnOr0UpapP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import unittest\n",
        "from parameterized import parameterized\n",
        "\n",
        "class TestSequence(unittest.TestCase):\n",
        "    @parameterized.expand([\n",
        "        [\"test1\", [0, 0, 0, 0], [0.25 for _ in range(4)],],\n",
        "        [\"test2\", [0, 0, 0, 0], [0.25 for _ in range(4)],],\n",
        "        [\"test3\", [1, 1, 1, 1], [0.25 for _ in range(4)],],\n",
        "        [\"test4\", [-np.inf, -np.inf, 1, 1], [0, 0, 0.5, 0.5],],\n",
        "        [\"test5\", [0, 1, 2, 3], [0.0320586 , 0.08714432, 0.23688282, 0.64391426],]\n",
        "    ])\n",
        "    def test_sequence(self, name, logits, expected_softmax):\n",
        "        logits = torch.Tensor(logits)\n",
        "        softmax = custom_softmax(logits)\n",
        "        print(softmax)\n",
        "        np.testing.assert_allclose(softmax, expected_softmax, atol=1e-4)\n",
        "\n",
        "\n",
        "unittest.main(argv=[''], verbosity=2, exit=False)"
      ],
      "metadata": {
        "id": "55zufrx_aKUN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}