{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem: Implement an RNN in PyTorch\n",
    "\n",
    "### Problem Statement\n",
    "You are tasked with implementing a **Recurrent Neural Network (RNN)** in PyTorch to process sequential data. The model should contain an RNN layer for handling sequential input and a fully connected layer to output the final predictions. Your goal is to complete the RNN model by defining the necessary layers and implementing the forward pass.\n",
    "\n",
    "### Requirements\n",
    "1. **Define the RNN Model**:\n",
    "   - Add an **RNN layer** to process sequential data.\n",
    "   - Add a **fully connected layer** to map the RNN output to the final prediction.\n",
    "\n",
    "### Constraints\n",
    "- Use appropriate configurations for the RNN layer, including hidden units and input/output sizes.\n",
    "\n",
    "\n",
    "<details>\n",
    "  <summary>ðŸ’¡ Hint</summary>\n",
    "  Add the RNN layer (self.rnn) and fully connected layer (self.fc) in RNNModel.__init__.\n",
    "  <br>\n",
    "  Implement the forward pass to process inputs through the RNN layer and fully connected layer.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic sequential data\n",
    "torch.manual_seed(42)\n",
    "sequence_length = 10\n",
    "num_samples = 100\n",
    "\n",
    "# Create a sine wave dataset\n",
    "X = torch.linspace(0, 4 * 3.14159, steps=num_samples).unsqueeze(1)\n",
    "y = torch.sin(X)\n",
    "\n",
    "# Prepare data for RNN\n",
    "def create_in_out_sequences(data, seq_length):\n",
    "    in_seq = []\n",
    "    out_seq = []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        in_seq.append(data[i:i + seq_length])\n",
    "        out_seq.append(data[i + seq_length])\n",
    "    return torch.stack(in_seq), torch.stack(out_seq)\n",
    "\n",
    "X_seq, y_seq = create_in_out_sequences(y, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RNN Model\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_dim=1, hidden_dim=50, output_dim=1):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Weight matrices for input and hidden state\n",
    "        self.W_ih = nn.Parameter(torch.randn(input_dim, hidden_dim) * 0.1)\n",
    "        self.W_hh = nn.Parameter(torch.randn(hidden_dim, hidden_dim) * 0.1)\n",
    "        self.b_h = nn.Parameter(torch.zeros(hidden_dim))\n",
    "\n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        # Activation\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        h_t = torch.zeros(batch_size, self.hidden_dim, device=x.device)\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            x_t = x[:, t, :]\n",
    "            h_t = self.tanh(x_t @ self.W_ih + h_t @ self.W_hh + self.b_h)\n",
    "\n",
    "        output = self.output_layer(h_t)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500], Loss: 0.0721\n",
      "Epoch [2/500], Loss: 0.0034\n",
      "Epoch [3/500], Loss: 0.0746\n",
      "Epoch [4/500], Loss: 0.0022\n",
      "Epoch [5/500], Loss: 0.0521\n",
      "Epoch [6/500], Loss: 0.0001\n",
      "Epoch [7/500], Loss: 0.0048\n",
      "Epoch [8/500], Loss: 0.0160\n",
      "Epoch [9/500], Loss: 0.0002\n",
      "Epoch [10/500], Loss: 0.0002\n",
      "Epoch [11/500], Loss: 0.0005\n",
      "Epoch [12/500], Loss: 0.0004\n",
      "Epoch [13/500], Loss: 0.0000\n",
      "Epoch [14/500], Loss: 0.0000\n",
      "Epoch [15/500], Loss: 0.0000\n",
      "Epoch [16/500], Loss: 0.0001\n",
      "Epoch [17/500], Loss: 0.0005\n",
      "Epoch [18/500], Loss: 0.0084\n",
      "Epoch [19/500], Loss: 0.0000\n",
      "Epoch [20/500], Loss: 0.0011\n",
      "Epoch [21/500], Loss: 0.0002\n",
      "Epoch [22/500], Loss: 0.0001\n",
      "Epoch [23/500], Loss: 0.0000\n",
      "Epoch [24/500], Loss: 0.0000\n",
      "Epoch [25/500], Loss: 0.0000\n",
      "Epoch [26/500], Loss: 0.0000\n",
      "Epoch [27/500], Loss: 0.0000\n",
      "Epoch [28/500], Loss: 0.0000\n",
      "Epoch [29/500], Loss: 0.0000\n",
      "Epoch [30/500], Loss: 0.0000\n",
      "Epoch [31/500], Loss: 0.0000\n",
      "Epoch [32/500], Loss: 0.0000\n",
      "Epoch [33/500], Loss: 0.0001\n",
      "Epoch [34/500], Loss: 0.0001\n",
      "Epoch [35/500], Loss: 0.0001\n",
      "Epoch [36/500], Loss: 0.0001\n",
      "Epoch [37/500], Loss: 0.0001\n",
      "Epoch [38/500], Loss: 0.0001\n",
      "Epoch [39/500], Loss: 0.0001\n",
      "Epoch [40/500], Loss: 0.0002\n",
      "Epoch [41/500], Loss: 0.0002\n",
      "Epoch [42/500], Loss: 0.0002\n",
      "Epoch [43/500], Loss: 0.0002\n",
      "Epoch [44/500], Loss: 0.0003\n",
      "Epoch [45/500], Loss: 0.0003\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Add batch dimension\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\chand\\miniconda3\\envs\\torchleet\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\chand\\miniconda3\\envs\\torchleet\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[3], line 24\u001b[0m, in \u001b[0;36mRNNModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(seq_len):\n\u001b[0;32m     23\u001b[0m     x_t \u001b[38;5;241m=\u001b[39m x[:, t, :]\n\u001b[1;32m---> 24\u001b[0m     h_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtanh(x_t \u001b[38;5;241m@\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_ih \u001b[38;5;241m+\u001b[39m h_t \u001b[38;5;241m@\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_hh \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mb_h\u001b[49m)\n\u001b[0;32m     26\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_layer(h_t)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\Users\\chand\\miniconda3\\envs\\torchleet\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1915\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1910\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[0;32m   1912\u001b[0m \u001b[38;5;66;03m# It is crucial that the return type is not annotated as `Any`, otherwise type checking\u001b[39;00m\n\u001b[0;32m   1913\u001b[0m \u001b[38;5;66;03m# on `torch.nn.Module` and all its subclasses is largely disabled as a result. See:\u001b[39;00m\n\u001b[0;32m   1914\u001b[0m \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/115074\u001b[39;00m\n\u001b[1;32m-> 1915\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModule\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m   1916\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[0;32m   1917\u001b[0m         _parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "model = RNNModel()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 500\n",
    "for epoch in range(epochs):\n",
    "    for sequences, labels in zip(X_seq, y_seq):\n",
    "        sequences = sequences.unsqueeze(0)  # Add batch dimension\n",
    "        labels = labels.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(sequences)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for new sequence: [[2.7525057792663574]]\n"
     ]
    }
   ],
   "source": [
    "# Testing on new data\n",
    "X_test = torch.linspace(4 * 3.14159, 5 * 3.14159, steps=10).unsqueeze(1)\n",
    "\n",
    "# Reshape to (batch_size, sequence_length, input_size)\n",
    "X_test = X_test.unsqueeze(0)  # Add batch dimension, shape becomes (1, 10, 1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_test)\n",
    "    print(f\"Predictions for new sequence: {predictions.tolist()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
