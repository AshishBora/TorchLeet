{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zl9a8mm4ajMX"
      },
      "source": [
        "## Problem: Quantize Your Language Model\n",
        "\n",
        "### Problem Statement\n",
        "Implement a **language model** using an LSTM and apply **dynamic quantization** to optimize it for inference. Dynamic quantization reduces the model size and enhances inference speed by quantizing the weights of the model.\n",
        "\n",
        "### Requirements\n",
        "\n",
        "1. **Define the Language Model**:\n",
        "   - **Purpose**: Build a simple language model that predicts the next token in a sequence.\n",
        "   - **Components**:\n",
        "     - **Embedding Layer**: Converts input tokens into dense vector representations.\n",
        "     - **LSTM Layer**: Processes the embedded sequence to capture temporal dependencies.\n",
        "     - **Fully Connected Layer**: Outputs predictions for the next token.\n",
        "     - **Softmax Layer**: Applies a probability distribution over the vocabulary for predictions.\n",
        "   - **Forward Pass**:\n",
        "     - Pass the input sequence through the embedding layer.\n",
        "     - Feed the embedded sequence into the LSTM.\n",
        "     - Use the final hidden state from the LSTM to make predictions via the fully connected layer.\n",
        "     - Apply the softmax function to obtain probabilities over the vocabulary.\n",
        "\n",
        "2. **Apply Dynamic Quantization**:\n",
        "   - Quantize the model dynamically\n",
        "   - Evaluate the quantized model's performance compared to the original model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mBfvghePajMa"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.quantization import quantize_dynamic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "RKDNjRXMajMb",
        "outputId": "e190a58e-6538-41ab-844a-0384ca27f477",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5] - Loss: 3.9426\n",
            "Epoch [2/5] - Loss: 3.9332\n",
            "Epoch [3/5] - Loss: 3.9239\n",
            "Epoch [4/5] - Loss: 3.9146\n",
            "Epoch [5/5] - Loss: 3.9053\n"
          ]
        }
      ],
      "source": [
        "# TODO: Define a simple Language Model (an LSTM-based model)\n",
        "class LSTMLayer(nn.Module):\n",
        "    # one layer of LSTM which maps from input_size to input_size with\n",
        "    # hidden state of hidden_size.\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.linear_o = nn.Linear(input_size + hidden_size, input_size)\n",
        "        self.linear_h = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        B, T, _ = inputs.shape\n",
        "        h_t = torch.zeros(B, self.hidden_size)\n",
        "        outputs = []\n",
        "        for t in range(T):\n",
        "            merged = torch.cat([inputs[:, t, :], h_t], dim=-1) # (B, T, input_size + hidden_size)\n",
        "            o_t = self.linear_o(merged)\n",
        "            h_t = self.linear_h(merged)\n",
        "            outputs.append(o_t)\n",
        "        return torch.stack(outputs)  # (B, T, input_size=output_size)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class LanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
        "        super(LanguageModel, self).__init__()\n",
        "        self.embedding = torch.nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = [LSTMLayer(embed_size, hidden_size) for _ in range(num_layers)]\n",
        "        self.linear = nn.Linear(embed_size, vocab_size)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, T)\n",
        "        x = self.embedding(x) # (B, T, embed_size)\n",
        "        for layer in self.lstm:\n",
        "            x = layer(x)\n",
        "        # LSTM output: (B, T, vocab_size)\n",
        "        x = x[:, -1, :] # extract the last output\n",
        "        x = self.linear(x)  # (B, vocab_size) = logits\n",
        "        return x\n",
        "\n",
        "\n",
        "# Create synthetic training data\n",
        "torch.manual_seed(42)\n",
        "vocab_size = 50\n",
        "seq_length = 10\n",
        "batch_size = 32\n",
        "X_train = torch.randint(0, vocab_size, (batch_size, seq_length))  # Random integer input\n",
        "y_train = torch.randint(0, vocab_size, (batch_size,))  # Random target words\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "embed_size = 64\n",
        "hidden_size = 128\n",
        "num_layers = 2\n",
        "model = LanguageModel(vocab_size, embed_size, hidden_size, num_layers)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "# Training loop\n",
        "epochs = 5\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    output = model(X_train)\n",
        "    loss = criterion(output, y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Log progress every epoch\n",
        "    print(f\"Epoch [{epoch + 1}/{epochs}] - Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Now, we will quantize the model dynamically to reduce its size and improve inference speed\n",
        "# Quantization: Apply dynamic quantization to the language model\n",
        "quantized_model = quantize_dynamic(model, {nn.Linear, nn.LSTM}, dtype=torch.qint8)\n",
        "\n",
        "# Save the quantized model\n",
        "torch.save(quantized_model.state_dict(), \"quantized_language_model.pth\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "RUaOH22VajMd",
        "outputId": "db55ae8a-a5e3-41b8-a7c9-4507b972ad96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/_utils.py:410: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  device=storage.device,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "# Load the quantized model and test it\n",
        "quantized_model = LanguageModel(vocab_size, embed_size, hidden_size, num_layers)\n",
        "\n",
        "# Apply dynamic quantization on the model after defining it\n",
        "quantized_model = quantize_dynamic(quantized_model, {nn.Linear, nn.LSTM}, dtype=torch.qint8)\n",
        "\n",
        "quantized_model.load_state_dict(torch.load(\"quantized_language_model.pth\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "-_p79mUvajMe",
        "outputId": "b7d5de98-8178-4539-d21a-a1f64a877ca2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction for input [[35, 17, 8, 32, 37, 45, 20, 29, 21, 20]]: 36\n"
          ]
        }
      ],
      "source": [
        "# Testing the quantized model on a sample input\n",
        "quantized_model.eval()\n",
        "test_input = torch.randint(0, vocab_size, (1, seq_length))\n",
        "with torch.no_grad():\n",
        "    prediction = quantized_model(test_input)\n",
        "    print(f\"Prediction for input {test_input.tolist()}: {prediction.argmax(dim=1).item()}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}