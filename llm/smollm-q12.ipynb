{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "746ece12",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64b63728",
   "metadata": {},
   "source": [
    "## Question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a697924",
   "metadata": {},
   "source": [
    "Code SmolLM-135M from scratch. (Architecture defined below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4515dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# smolLM(\n",
    "#   (model): smolModel(\n",
    "#     (embed_tokens): Embedding(49152, 576)\n",
    "#     (layers): ModuleList(\n",
    "#       (0-29): 30 x LlamaDecoder(\n",
    "#         (self_attn): RopeAttention(\n",
    "#           (W_query): Linear(in_features=576, out_features=576, bias=False)\n",
    "#           (W_key): Linear(in_features=576, out_features=192, bias=False)\n",
    "#           (W_value): Linear(in_features=576, out_features=192, bias=False)\n",
    "#           (W_output): Linear(in_features=576, out_features=576, bias=False)\n",
    "#           (rotary_emb): RotaryEmbedder()\n",
    "#         )\n",
    "#         (mlp): MLP(\n",
    "#           (W_gate): Linear(in_features=576, out_features=1536, bias=False)\n",
    "#           (W_up): Linear(in_features=576, out_features=1536, bias=False)\n",
    "#           (W_down): Linear(in_features=1536, out_features=576, bias=False)\n",
    "#           (act_fn): SiLU()\n",
    "#         )\n",
    "#         (pre_attn_rmsnorm): RMSNorm()\n",
    "#         (pre_mlp_rmsnorm): RMSNorm()\n",
    "#       )\n",
    "#     )\n",
    "#     (norm): RMSNorm()\n",
    "#   )\n",
    "#   (lm_head): Linear(in_features=576, out_features=49152, bias=False)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5933189",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d6e84d3",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3da31db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Helper function to rotate the last half of a tensor\n",
    "# Used in rotary positional embeddings to compute sine and cosine rotations\n",
    "def rotate_half(x):\n",
    "    # Split tensor into two halves along the last dimension\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    # Rotate: negate the second half and concatenate it with the first half\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "# Applies rotary positional embeddings to query (q) and key (k) tensors\n",
    "# Uses sine and cosine positional encodings to enhance positional awareness\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n",
    "    # Expand cos and sin tensors for broadcasting\n",
    "    cos = cos.unsqueeze(unsqueeze_dim)\n",
    "    sin = sin.unsqueeze(unsqueeze_dim)\n",
    "\n",
    "    # Apply rotations to q and k using cos and sin\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed\n",
    "\n",
    "# Repeats key-value tensors for multiple attention heads\n",
    "# Ensures compatibility between the number of attention heads and key-value heads\n",
    "def repeat_kv(hidden_states, n_rep):\n",
    "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
    "    # Expand the number of key-value heads by repeating them\n",
    "    hidden_states = hidden_states[:, :, None, :, :].expand(\n",
    "        batch, num_key_value_heads, n_rep, slen, head_dim\n",
    "    )\n",
    "    # Reshape to align with the expected multi-head attention format\n",
    "    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n",
    "\n",
    "# Computes rotary positional embeddings for queries and keys\n",
    "class RotaryEmbedder(nn.Module):\n",
    "    def __init__(self, dim, base):\n",
    "        super().__init__()\n",
    "        # Precompute frequency for sine/cosine embeddings\n",
    "        self.freq = 1.0 / (base ** (torch.arange(0, dim, 2, dtype=torch.float32) / dim))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x):\n",
    "        # Generate positions (sequence indices) for the input\n",
    "        pos = torch.arange(x.shape[-2], dtype=torch.long)\n",
    "        # Compute angles for sine and cosine embeddings\n",
    "        angles = torch.einsum(\"p,f->pf\", pos.float(), self.freq).unsqueeze(dim=0)\n",
    "        # Duplicate angles for sine and cosine embeddings\n",
    "        emb = torch.cat((angles, angles), dim=-1)\n",
    "        # Return cosine and sine components of the positional embeddings\n",
    "        return emb.cos(), emb.sin()\n",
    "\n",
    "# Implements attention with rotary positional embeddings\n",
    "class RopeAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # Model dimensions and attention configurations\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_heads = config.num_heads\n",
    "        self.head_dim = config.hidden_size // self.num_heads\n",
    "        self.kv_heads = config.kv_heads  # Number of key-value heads\n",
    "        self.rope_theta = 10000.0  # Scaling factor for rotary embeddings\n",
    "\n",
    "        # Linear projections for queries, keys, values, and output\n",
    "        self.W_query = nn.Linear(config.hidden_size, self.num_heads * self.head_dim, bias=False)\n",
    "        self.W_key = nn.Linear(config.hidden_size, self.kv_heads * self.head_dim, bias=False)\n",
    "        self.W_value = nn.Linear(config.hidden_size, self.kv_heads * self.head_dim, bias=False)\n",
    "        self.W_output = nn.Linear(config.hidden_size, config.hidden_size, bias=False)\n",
    "\n",
    "        # Rotary embedding generator\n",
    "        self.rotary_emb = RotaryEmbedder(base=self.rope_theta, dim=self.head_dim)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor, attention_mask=None):\n",
    "        # Input dimensions: (batch_size, seq_len, hidden_size)\n",
    "        b, q, _ = hidden_states.size()\n",
    "\n",
    "        # Project input hidden states into queries, keys, and values\n",
    "        q_states = self.W_query(hidden_states)\n",
    "        k_states = self.W_key(hidden_states)\n",
    "        v_states = self.W_value(hidden_states)\n",
    "\n",
    "        # Reshape and transpose for multi-head attention\n",
    "        q_states = q_states.view(b, q, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k_states = k_states.view(b, q, self.kv_heads, self.head_dim).transpose(1, 2)\n",
    "        v_states = v_states.view(b, q, self.kv_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Compute rotary positional embeddings\n",
    "        cos, sin = self.rotary_emb(q_states)\n",
    "        # Apply positional embeddings to queries and keys\n",
    "        q_states, k_states = apply_rotary_pos_emb(q_states, k_states, cos, sin)\n",
    "\n",
    "        # Repeat key and value tensors to match the number of query heads\n",
    "        __kv_groups = self.num_heads // self.kv_heads\n",
    "        k_states = repeat_kv(k_states, __kv_groups)\n",
    "        v_states = repeat_kv(v_states, __kv_groups)\n",
    "\n",
    "        # Compute attention scores (scaled dot-product attention)\n",
    "        attn_weights = torch.matmul(q_states, k_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "\n",
    "        # Add attention mask (e.g., for causal or padding masking)\n",
    "        attn_weights = attn_weights + attention_mask\n",
    "\n",
    "        # Normalize attention weights using softmax\n",
    "        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
    "        # Apply dropout to attention weights\n",
    "        attn_weights = nn.functional.dropout(attn_weights, 0)\n",
    "\n",
    "        # Compute attention output\n",
    "        attn_output = torch.matmul(attn_weights, v_states)\n",
    "        # Reshape and transpose back to original format\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "        attn_output = attn_output.reshape(b, q, -1)\n",
    "\n",
    "        # Project the attention output back to the hidden size\n",
    "        attn_output = self.W_output(attn_output)\n",
    "\n",
    "        # Return the final attention output\n",
    "        return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "574659e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Multi-Layer Perceptron (MLP) class\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_size, intermediate_size):\n",
    "        \"\"\"\n",
    "        Initialize an MLP with a gating mechanism and non-linear activation.\n",
    "\n",
    "        Args:\n",
    "            hidden_size (int): The size of the input and output embeddings.\n",
    "            intermediate_size (int): The size of the hidden layer.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.intermediate_size = intermediate_size\n",
    "\n",
    "        # Linear layers for the gated MLP structure\n",
    "        self.W_gate = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n",
    "        self.W_up = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n",
    "        self.W_down = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n",
    "\n",
    "        # Activation function (SiLU)\n",
    "        self.act_fn = torch.nn.modules.activation.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for the gated MLP.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, seq_len, hidden_size).\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch_size, seq_len, hidden_size).\n",
    "        \"\"\"\n",
    "        # Apply gating mechanism and project back to hidden size\n",
    "        down_proj = self.W_down(self.act_fn(self.W_gate(x)) * self.W_up(x))\n",
    "        return down_proj\n",
    "\n",
    "\n",
    "# RMSNorm class (Root Mean Square Normalization)\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, eps=1e-6):\n",
    "        \"\"\"\n",
    "        Initialize RMSNorm.\n",
    "\n",
    "        Args:\n",
    "            hidden_size (int): The size of the input embeddings.\n",
    "            eps (float): A small value to prevent division by zero.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))  # Learnable scaling factor\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        \"\"\"\n",
    "        Forward pass for RMSNorm.\n",
    "\n",
    "        Args:\n",
    "            hidden_states (torch.Tensor): Input tensor of shape (batch_size, seq_len, hidden_size).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Normalized tensor.\n",
    "        \"\"\"\n",
    "        # Calculate variance along the last dimension (hidden size)\n",
    "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
    "\n",
    "        # Normalize and scale\n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
    "        return self.weight * hidden_states\n",
    "\n",
    "\n",
    "# Rotary Embedder class\n",
    "class RotaryEmbedder(nn.Module):\n",
    "    def __init__(self, dim, base):\n",
    "        \"\"\"\n",
    "        Initialize rotary embeddings for positional encodings.\n",
    "\n",
    "        Args:\n",
    "            dim (int): Dimensionality of embeddings (half the hidden size per head).\n",
    "            base (float): Base frequency for rotary embeddings.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Frequency for rotary embeddings\n",
    "        self.freq = 1.0 / (base ** (torch.arange(0, dim, 2, dtype=torch.float32) / dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Compute cosine and sine embeddings for rotary position encoding.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, seq_len, dim).\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor]: Cosine and sine embeddings.\n",
    "        \"\"\"\n",
    "        pos = torch.arange(x.shape[-2], dtype=torch.float32)  # Sequence positions\n",
    "\n",
    "        # Calculate angular frequencies\n",
    "        angles = torch.einsum(\"p,f->pf\", pos, self.freq).unsqueeze(0)\n",
    "        \n",
    "        # Create cosine and sine embeddings\n",
    "        emb = torch.cat((angles, angles), dim=-1)\n",
    "        return emb.cos(), emb.sin()\n",
    "\n",
    "\n",
    "# LlamaDecoder class\n",
    "class LlamaDecoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        Initialize the LlamaDecoder layer with attention and MLP sublayers.\n",
    "\n",
    "        Args:\n",
    "            config: Configuration object containing model parameters.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Attention mechanism with rotary embeddings\n",
    "        self.self_attn = RopeAttention(config)\n",
    "\n",
    "        # Feedforward neural network (MLP)\n",
    "        self.mlp = MLP(hidden_size=config.hidden_size, intermediate_size=config.intermediate_size)\n",
    "\n",
    "        # Pre-layer normalization (RMSNorm) for attention and MLP\n",
    "        self.pre_attn_rmsnorm = RMSNorm(config.hidden_size, eps=1e-05)\n",
    "        self.pre_mlp_rmsnorm = RMSNorm(config.hidden_size, eps=1e-05)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask):\n",
    "        \"\"\"\n",
    "        Forward pass for the LlamaDecoder layer.\n",
    "\n",
    "        Args:\n",
    "            hidden_states (torch.Tensor): Input tensor of shape (batch_size, seq_len, hidden_size).\n",
    "            attention_mask (torch.Tensor): Mask to prevent attention to certain positions.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor]: Output tensor of shape (batch_size, seq_len, hidden_size).\n",
    "        \"\"\"\n",
    "        # Residual connection for attention sublayer\n",
    "        residual = hidden_states\n",
    "\n",
    "        # Apply RMSNorm before attention\n",
    "        hidden_states = self.pre_attn_rmsnorm(hidden_states)\n",
    "\n",
    "        # Generate a triangular attention mask (causal masking)\n",
    "        attention_mask = torch.triu(torch.full((attention_mask.shape[-1], attention_mask.shape[-1]),\n",
    "                                               fill_value=float('-inf')), diagonal=1)\n",
    "\n",
    "        # Apply self-attention\n",
    "        hidden_states = self.self_attn(\n",
    "            hidden_states=hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "\n",
    "        # Add residual connection\n",
    "        hidden_states += residual\n",
    "\n",
    "        # Residual connection for MLP sublayer\n",
    "        residual = hidden_states\n",
    "\n",
    "        # Apply RMSNorm before MLP\n",
    "        hidden_states = self.pre_mlp_rmsnorm(hidden_states)\n",
    "\n",
    "        # Pass through MLP\n",
    "        hidden_states = self.mlp(hidden_states)\n",
    "\n",
    "        # Add residual connection\n",
    "        hidden_states += residual\n",
    "\n",
    "        # Return the output hidden states\n",
    "        return hidden_states,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2d2f9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# The main model containing the embedding layer, decoder stack, and normalization layer\n",
    "class smolModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # Token embedding layer: maps input tokens to dense representations\n",
    "        self.embed_tokens = nn.Embedding(\n",
    "            num_embeddings=config.vocab_size,\n",
    "            embedding_dim=config.hidden_size\n",
    "        )\n",
    "        # Stack of decoder layers (LlamaDecoder) defined by the configuration\n",
    "        self.layers = nn.ModuleList([\n",
    "            LlamaDecoder(config) for _ in range(config.num_hidden_layers)\n",
    "        ])\n",
    "        # RMSNorm: final layer normalization applied to hidden states\n",
    "        self.norm = RMSNorm(config.hidden_size, eps=1e-05)\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None):\n",
    "        # Convert input token IDs to dense embeddings\n",
    "        inputs_embeds = self.embed_tokens(input_ids)\n",
    "        hidden_states = inputs_embeds\n",
    "\n",
    "        # Pass embeddings through each decoder layer in the stack\n",
    "        for decoder_layer in self.layers:\n",
    "            layer_outputs = decoder_layer(\n",
    "                hidden_states,\n",
    "                attention_mask=attention_mask,  # Pass the attention mask\n",
    "            )\n",
    "            # Update hidden states with the output of the current decoder layer\n",
    "            hidden_states = layer_outputs[0]\n",
    "\n",
    "        # Apply final layer normalization to the hidden states\n",
    "        hidden_states = self.norm(hidden_states)\n",
    "\n",
    "        # Return the processed hidden states\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "# The complete language model, combining smolModel and a language modeling head (lm_head)\n",
    "class smolLM(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # Core model containing embeddings and decoder stack\n",
    "        self.model = smolModel(config)\n",
    "        # Language modeling head: projects hidden states to vocabulary logits\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "        # Tie weights between the embedding layer and lm_head\n",
    "        self.tie_weights()\n",
    "\n",
    "    def tie_weights(self):\n",
    "        # Ensures the lm_head shares weights with the embedding layer\n",
    "        # This is a common optimization for language models\n",
    "        self.lm_head.weight = self.model.embed_tokens.weight\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Pass inputs through the core model to obtain hidden states\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "\n",
    "        # Obtain hidden states from the model's output\n",
    "        hidden_states = outputs\n",
    "\n",
    "        # Pass hidden states through the language modeling head\n",
    "        logits = self.lm_head(hidden_states)\n",
    "        # Ensure logits are returned in float for numerical stability\n",
    "        logits = logits.float()\n",
    "\n",
    "        # Return the output as a dictionary containing logits\n",
    "        return {'logits': logits}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5083c3ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e796bcb0",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "486d9159",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bargav/anaconda3/envs/llm_env/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>>>>>>>>>>>>>>\n",
      "\tPrompt\n",
      "<<<<<<<<<<<<<<<<<<<<\n",
      "Given the following film movie by a critic, rate it out of 10. Respond in a single number.\n",
      "\n",
      "The movie started off extremely well, but just got worse after that.\n",
      "The storyline was all over the place and everyone acted terribly.\n",
      " 10/10 would not recommend! \n",
      "\n",
      " \n",
      "\n",
      "\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\tModel_A Generation\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1\n",
      "\n",
      "\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\tModel_B Generation\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "# Libraries\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import math\n",
    "\n",
    "########################## HELPER FUNCTIONS ######################\n",
    "\n",
    "def __generate(model, inputs, num_tokens, tokenizer, max_length=50):\n",
    "    collect = []\n",
    "    for _ in range(num_tokens):\n",
    "        output = model(**inputs)\n",
    "        output_id = torch.argmax(output['logits'][0, -1]).item()\n",
    "        collect.append(output_id)\n",
    "        if output_id == tokenizer.eos_token_id or len(collect) >= max_length:\n",
    "            break\n",
    "        # Update input_ids and attention_mask\n",
    "        new_token = torch.tensor([output_id], device=inputs['input_ids'].device)\n",
    "        inputs['input_ids'] = torch.cat([inputs['input_ids'][0], new_token]).unsqueeze(0)\n",
    "        inputs['attention_mask'] = F.pad(inputs['attention_mask'], (0, 1), value=1)\n",
    "    return tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(collect))\n",
    "\n",
    "\n",
    "def check_solution(prompt, num_tokens, model_A, model_B, tokenizer, max_length=50):\n",
    "    print(f\"{'>'*20}\\n\\tPrompt\\n{'<'*20}\\n{prompt}\\n\\n\")\n",
    "    \n",
    "    model_inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    \n",
    "    try:\n",
    "        print(f\"{'>'*30}\\n\\tModel_A Generation\\n{'<'*30}\")\n",
    "        print(__generate(model_A, model_inputs, num_tokens, tokenizer, max_length))\n",
    "    except Exception as e:\n",
    "        print(f\"Error with Model_A: {e}\")\n",
    "    \n",
    "    try:\n",
    "        model_inputs = tokenizer(prompt, return_tensors='pt')\n",
    "        print(f\"\\n\\n{'>'*30}\\n\\tModel_B Generation\\n{'<'*30}\")\n",
    "        print(__generate(model_B, model_inputs, num_tokens, tokenizer, max_length))\n",
    "    except Exception as e:\n",
    "        print(f\"Error with Model_B: {e}\")\n",
    "\n",
    "\n",
    "class smolConfig:\n",
    "    vocab_size = 49152\n",
    "    hidden_size = 576\n",
    "    intermediate_size = 1536\n",
    "    num_hidden_layers = 30\n",
    "    num_heads = 9\n",
    "    kv_heads = 3\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "# Load tokenizer and reference model\n",
    "checkpoint = \"HuggingFaceTB/SmolLM-135M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "reference_model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
    "\n",
    "# Initialize smolLM\n",
    "config = smolConfig()\n",
    "test_model = smolLM(config)\n",
    "\n",
    "# Load weights\n",
    "state_dict = torch.load(\"../../temp/BareBones_SmolLM-135M.pt\")\n",
    "test_model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "check_solution(prompt=\"Given the following film movie by a critic, rate it out of 10. Respond in a single number.\\n\\nThe movie started off extremely well, but just got worse after that.\\nThe storyline was all over the place and everyone acted terribly.\\n 10/10 would not recommend! \\n\\n \",\n",
    "               num_tokens=1,\n",
    "               model_A=reference_model,\n",
    "               model_B=test_model, tokenizer=tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6492e7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb515f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
