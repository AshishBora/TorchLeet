{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yM02Vve_HEiK"
      },
      "source": [
        "# Implement Attention from Scratch\n",
        "### Problem Statement\n",
        "Implement a **Scaled Dot-Product Attention** mechanism from scratch using PyTorch. Your mission (should you choose to accept it) is to replicate what PyTorch's built-in `scaled_dot_product_attention` does ‚Äî manually. This core component is essential in Transformer architectures and helps models focus on relevant parts of a sequence. You'll test your implementation against PyTorch's native one to ensure you nailed it.\n",
        "\n",
        "\n",
        "### Requirements\n",
        "1. **Define the Function**:\n",
        "   - Create a function `scaled_dot_product_attention(q, k, v, mask=None)` that:\n",
        "     - Computes attention scores via the dot product of query and key vectors.\n",
        "     - Scales the scores using the square root of the key dimension.\n",
        "     - Applies an optional mask to the scores.\n",
        "     - Applies softmax to convert scores into attention weights.\n",
        "     - Uses these weights to compute a weighted sum of values (V).\n",
        "\n",
        "2. **Test Your Work**:\n",
        "   - Use sample tensors for query (Q), key (K), and value (V).\n",
        "   - Compare the result of your custom implementation with PyTorch's `F.scaled_dot_product_attention` using an `assert` to check numerical accuracy.\n",
        "\n",
        "\n",
        "### Constraints\n",
        "- ‚ùå Do NOT use `F.scaled_dot_product_attention` inside your custom function ‚Äî that defeats the whole point.\n",
        "- ‚úÖ Your implementation must handle **batch dimensions** correctly.\n",
        "- ‚úÖ Support optional **masking** for future tokens or padding.\n",
        "- ‚úÖ Use only PyTorch ops ‚Äî no cheating with external attention libs.\n",
        "\n",
        "\n",
        "\n",
        "<details>\n",
        "  <summary>üí° Hint</summary>\n",
        "  Use `torch.matmul()` to compute dot products and `F.softmax()` for the final attention weights. The mask (if used) should be applied **before** the softmax using `masked_fill`.\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wVJBqxmSHEiL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "velt6nDkHEiL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "batch_size = 1\n",
        "seq_len = 3\n",
        "dim = 3\n",
        "\n",
        "q = torch.randn(batch_size, seq_len, dim)\n",
        "k = torch.randn(batch_size, seq_len, dim)\n",
        "v = torch.randn(batch_size, seq_len, dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Cjq86bLVHEiM",
        "outputId": "3397aafd-78de-41d5-ba5d-3abde5797461",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.0377, -0.3133,  0.8707],\n",
            "         [ 0.4541, -0.3508,  0.8461],\n",
            "         [ 0.3709, -0.2885,  0.8044]]])\n",
            "tensor([[[ 0.0377, -0.3133,  0.8707],\n",
            "         [ 0.4541, -0.3508,  0.8461],\n",
            "         [ 0.3709, -0.2885,  0.8044]]])\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def scaled_dot_product_attention(q, k, v, mask=None):\n",
        "    \"\"\"\n",
        "    Compute the scaled dot-product attention.\n",
        "\n",
        "    Args:\n",
        "        q: Query tensor of shape (..., seq_len_q, d_k)\n",
        "        k: Key tensor of shape (..., seq_len_k, d_k)\n",
        "        v: Value tensor of shape (..., seq_len_k, d_v)\n",
        "        mask: Optional mask tensor of shape (..., seq_len_q, seq_len_k)\n",
        "\n",
        "    Returns:\n",
        "        output: Attention output tensor of shape (..., seq_len_q, d_v)\n",
        "        attention_weights: Attention weights tensor of shape (..., seq_len_q, seq_len_k)\n",
        "    \"\"\"\n",
        "    scores = torch.einsum('btd,bsd->bts', q, k) / np.sqrt(q.shape[-1])\n",
        "    if mask:\n",
        "        scores = torch.where(mask==0, -torch.inf, scores)\n",
        "    weights = torch.softmax(scores, dim=-1) # bts\n",
        "    output = torch.einsum('bts,bsd->btd', weights, v)\n",
        "    return output, weights\n",
        "\n",
        "\n",
        "# Testing on data & compare\n",
        "output_custom, _ = scaled_dot_product_attention(q, k, v)\n",
        "print(output_custom)\n",
        "output = F.scaled_dot_product_attention(q, k, v)\n",
        "print(output)\n",
        "\n",
        "assert torch.allclose(output_custom, output, atol=1e-08, rtol=1e-05) # Check if they are close enough."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DvQx7V2qIiuo"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}