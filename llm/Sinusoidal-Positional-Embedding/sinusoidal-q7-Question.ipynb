{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Attention from Scratch\n",
    "###  Problem Statement\n",
    "Transformers are order-agnostic ‚Äî they see tokens like goldfish: no sense of sequence. To inject **position awareness** into the model, we use **Sinusoidal Positional Embeddings**, where each position in the sequence gets a unique deterministic vector. These vectors are computed using sine and cosine waves at different frequencies.\n",
    "\n",
    "Your task is to implement the sinusoidal position encoding mechanism from scratch using PyTorch ‚Äî no cheating with built-ins from `fairseq` or Hugging Face.\n",
    "\n",
    "---\n",
    "\n",
    "###  Requirements\n",
    "\n",
    "1. **Define the Sinusoidal Embedding Class**\n",
    "   - Implement a `SinusoidalPositionalEmbedding` class inheriting from `nn.Module`.\n",
    "   - Initialize with `max_seq_len` and `d_model`.\n",
    "   - Create a tensor `pe` of shape `(max_seq_len, d_model)` filled with sine and cosine encodings:\n",
    "     - `sin(position * œâ)` for even indices\n",
    "     - `cos(position * œâ)` for odd indices\n",
    "\n",
    "2. **Register as Buffer**\n",
    "   - Use `self.register_buffer(\"pe\", pe)` to store `pe` without treating it as a trainable parameter.\n",
    "\n",
    "3. **Generate Encodings**\n",
    "   - On calling `forward(x)`, return the slice of positional encodings matching the sequence length of `x`.\n",
    "\n",
    "4. **Test the Embeddings**\n",
    "   - Initialize the embedding class with `max_seq_len = 100` and `d_model = 64`.\n",
    "   - Pass a sequence of length 50 to verify the returned shape is `(1, 50, 64)`.\n",
    "\n",
    "---\n",
    "\n",
    "### Constraints\n",
    "\n",
    "- ‚úÖ Do not use Hugging Face, Fairseq, or built-in PyTorch modules for position encoding.\n",
    "- ‚úÖ Ensure the `pe` tensor is not a trainable parameter.\n",
    "- ‚úÖ Support any sequence length up to `max_seq_len`.\n",
    "- ‚ùå Do not inject these embeddings directly into token embeddings yet ‚Äî this is just the embedding module.\n",
    "\n",
    "---\n",
    "\n",
    "<details>\n",
    "  <summary>üí° Hint</summary>\n",
    "\n",
    "  - Use `torch.arange(0, max_seq_len).unsqueeze(1)` to create position indices.\n",
    "  - Compute frequencies with `torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))`.\n",
    "  - Alternate `sin` and `cos` values for even and odd embedding dimensions.\n",
    "  - When returning the embedding in `forward`, use `.unsqueeze(0)` to broadcast over the batch dimension.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic data\n",
    "torch.manual_seed(42)\n",
    "batch_size = 3\n",
    "seq_len = 4\n",
    "d_model = 8\n",
    "num_heads = 2\n",
    "\n",
    "q = torch.rand(batch_size, seq_len, d_model)\n",
    "k = torch.rand(batch_size, seq_len, d_model)\n",
    "v = torch.rand(batch_size, seq_len, d_model)\n",
    "print(q.shape)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPositionalEmbedding(nn.Module):\n",
    "    def __init__(self, max_seq_len: int, d_model: int):\n",
    "        \"\"\"\n",
    "        Initializes the sinusoidal positional embedding.\n",
    "        \n",
    "        Args:\n",
    "            max_seq_len (int): Maximum sequence length.\n",
    "            d_model (int): Embedding dimension.\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Returns the positional embedding for a given input tensor.\n",
    "        \n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape (batch_size, seq_len, d_model).\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Positional embeddings of shape (batch_size, seq_len, d_model).\n",
    "        \"\"\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fairseq.modules.sinusoidal_positional_embedding import SinusoidalPositionalEmbedding\n",
    "\n",
    "max_seq_len = 100\n",
    "d_model = 64\n",
    "\n",
    "# Fairseq's implementation requires the number of embeddings (seq length) and embedding dim\n",
    "# pos_emb = SinusoidalPositionalEmbedding(d_model, max_seq_len, padding_idx=None)\n",
    "\n",
    "# Generate embeddings for a sequence of length 50\n",
    "seq_len = 50\n",
    "positions = torch.arange(seq_len).unsqueeze(0)  # Shape: (1, seq_len)\n",
    "# positional_encoding = pos_emb(positions)  # Shape: (1, seq_len, d_model)\n",
    "\n",
    "custom_pos_emb = SinusoidalPositionalEmbedding(d_model, max_seq_len)\n",
    "\n",
    "positional_encoding_custom = custom_pos_emb(positions)\n",
    "\n",
    "print(positional_encoding_custom.shape)  # (1, 50, 64)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
