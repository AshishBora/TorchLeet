{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vF0ILJUiJuIb"
      },
      "source": [
        "# Implement Attention from Scratch\n",
        "### Problem Statement\n",
        "Multi-Head Attention (MHA) is the bread-and-butter of the Transformer architecture. It enables the model to **jointly attend** to information from different representation subspaces at different positions.\n",
        "\n",
        "Your goal is to implement MHA from scratch using PyTorch, simulating exactly what `torch.nn.MultiheadAttention` does â€” projecting Q, K, V for each head, computing attention weights, applying them to V, and concatenating the outputs across all heads.\n",
        "\n",
        "---\n",
        "\n",
        "### Requirements\n",
        "\n",
        "1. **Linear Projections for Q, K, V**\n",
        "   - Project input `q`, `k`, `v` into a total of `d_model` dimensions.\n",
        "   - Split them into `num_heads` of `d_head = d_model // num_heads` each.\n",
        "\n",
        "2. **Scaled Dot-Product Attention per Head**\n",
        "   - Compute attention scores:  \n",
        "     `scores = Q @ Káµ€ / sqrt(d_head)`\n",
        "   - Apply an optional `mask` before softmax.\n",
        "   - Use the scores to weight `V`.\n",
        "\n",
        "3. **Combine the Heads**\n",
        "   - Concatenate the outputs of all heads.\n",
        "   - Apply a final linear projection to restore the shape: `(batch_size, seq_len, d_model)`.\n",
        "\n",
        "4. **Validate Against PyTorchâ€™s Reference**\n",
        "   - Test your output against `torch.nn.MultiheadAttention` using the same input tensors.\n",
        "   - Check for numerical closeness using `torch.allclose()`.\n",
        "\n",
        "---\n",
        "\n",
        "### Constraints\n",
        "\n",
        "- âœ… Use only PyTorch operations.\n",
        "- âœ… Make sure all tensors are reshaped properly when splitting and combining heads.\n",
        "- âœ… Support optional masking.\n",
        "- âœ… Must match `torch.nn.MultiheadAttention` output when heads and shape are aligned.\n",
        "\n",
        "---\n",
        "\n",
        "<details>\n",
        "  <summary>ðŸ’¡ Hint</summary>\n",
        "\n",
        "  - Use `.view()` and `.transpose()` to shape Q, K, V to `(batch_size, num_heads, seq_len, d_head)`.\n",
        "  - Softmax should be applied over the **last dimension** (attention scores across sequence).\n",
        "  - Use `.contiguous().view()` to flatten the multi-head outputs back into `(batch_size, seq_len, d_model)`.\n",
        "  - Match PyTorchâ€™s behavior using the same projections and batch-first format.\n",
        "\n",
        "</details>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_ySL2tpjJuIc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "EsxUeSt2JuId",
        "outputId": "36da788d-b749-490b-fac2-86ab5e9c9a45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 4, 8])\n"
          ]
        }
      ],
      "source": [
        "# Synthetic data\n",
        "torch.manual_seed(42)\n",
        "batch_size = 3\n",
        "seq_len = 4\n",
        "d_model = 8\n",
        "num_heads = 2\n",
        "\n",
        "q = torch.rand(batch_size, seq_len, d_model)\n",
        "k = torch.rand(batch_size, seq_len, d_model)\n",
        "v = torch.rand(batch_size, seq_len, d_model)\n",
        "print(q.shape)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device = \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "k5e72Q9CJuId"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def multi_head_attention(q, k, v, num_heads, d_model, mask=None):\n",
        "    \"\"\"\n",
        "    Implements multi-head attention.\n",
        "\n",
        "    Args:\n",
        "        q (Tensor): Query tensor of shape (batch_size, seq_len, d_model)\n",
        "        k (Tensor): Key tensor of shape (batch_size, seq_len, d_model)\n",
        "        v (Tensor): Value tensor of shape (batch_size, seq_len, d_model)\n",
        "        num_heads (int): Number of attention heads\n",
        "        d_model (int): Total embedding dimension\n",
        "        mask (Tensor, optional): Masking tensor for attention\n",
        "\n",
        "    Returns:\n",
        "        Tensor: Multi-head attention output of shape (batch_size, seq_len, d_model)\n",
        "    \"\"\"\n",
        "\n",
        "    b, t, d = q.shape\n",
        "    _, s, _ = k.shape\n",
        "    h = num_heads\n",
        "    q = q.reshape([b, t, h, d//h])\n",
        "    k = k.reshape([b, s, h, d//h])\n",
        "    v = v.reshape([b, s, h, d//h])\n",
        "\n",
        "    scores = torch.einsum('bthd,bshd->bhts', q, k) / np.sqrt(d//h)\n",
        "    if mask:\n",
        "        scores = torch.where(mask==0, -torch.inf, scores)\n",
        "    weights = scores.softmax(dim=-1)\n",
        "    output = torch.einsum('bhts,bshd->bthd', weights, v)\n",
        "    output = output.reshape([b, t, d])\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "i9Snb5hTJuId",
        "outputId": "5dd24d08-3b46-4514-c3f2-adc7943894dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 888
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0.4663, 0.5878, 0.7101, 0.5205, 0.3190, 0.4160, 0.3755, 0.5013],\n",
            "         [0.4711, 0.5907, 0.7079, 0.5186, 0.3179, 0.4094, 0.3754, 0.5044],\n",
            "         [0.4645, 0.5890, 0.7107, 0.5228, 0.3196, 0.4143, 0.3717, 0.4947],\n",
            "         [0.4525, 0.5736, 0.7196, 0.5246, 0.3156, 0.4096, 0.3670, 0.4936]],\n",
            "\n",
            "        [[0.4097, 0.6708, 0.6076, 0.4224, 0.3674, 0.1395, 0.1817, 0.3833],\n",
            "         [0.4157, 0.6726, 0.6049, 0.4202, 0.3819, 0.1433, 0.1792, 0.3662],\n",
            "         [0.4188, 0.6925, 0.6050, 0.4212, 0.3584, 0.1340, 0.1780, 0.4003],\n",
            "         [0.4071, 0.6927, 0.6012, 0.4161, 0.3792, 0.1406, 0.1765, 0.3718]],\n",
            "\n",
            "        [[0.6554, 0.5052, 0.6553, 0.4615, 0.4144, 0.2952, 0.6128, 0.2900],\n",
            "         [0.6561, 0.5060, 0.6351, 0.4754, 0.4194, 0.2955, 0.6165, 0.2900],\n",
            "         [0.6594, 0.5299, 0.6568, 0.4584, 0.3994, 0.2991, 0.6167, 0.2753],\n",
            "         [0.6555, 0.5199, 0.6562, 0.4559, 0.4063, 0.2945, 0.6198, 0.2749]]])\n",
            "tensor([[[ 7.1700e-02,  5.3411e-02,  3.3034e-01,  1.5892e-01,  6.5924e-02,\n",
            "           2.3432e-01,  1.2263e-01,  2.1005e-01],\n",
            "         [ 7.5689e-02,  5.2544e-02,  3.3219e-01,  1.5638e-01,  6.8600e-02,\n",
            "           2.3832e-01,  1.2223e-01,  2.1144e-01],\n",
            "         [ 7.2534e-02,  5.3125e-02,  3.3081e-01,  1.5864e-01,  6.6714e-02,\n",
            "           2.3542e-01,  1.2263e-01,  2.1010e-01],\n",
            "         [ 6.9490e-02,  5.1792e-02,  3.3143e-01,  1.6153e-01,  6.6810e-02,\n",
            "           2.3345e-01,  1.2220e-01,  2.0941e-01]],\n",
            "\n",
            "        [[-4.5193e-02,  2.6360e-02,  2.6671e-01,  2.7284e-01,  9.3438e-02,\n",
            "           1.5845e-01,  1.2320e-01,  1.5793e-01],\n",
            "         [-4.3427e-02,  2.6275e-02,  2.6286e-01,  2.7077e-01,  9.2779e-02,\n",
            "           1.5760e-01,  1.2343e-01,  1.5535e-01],\n",
            "         [-4.5840e-02,  2.6753e-02,  2.6641e-01,  2.7329e-01,  9.4146e-02,\n",
            "           1.5864e-01,  1.2388e-01,  1.5618e-01],\n",
            "         [-4.1810e-02,  2.5378e-02,  2.6161e-01,  2.6910e-01,  9.1674e-02,\n",
            "           1.5723e-01,  1.2250e-01,  1.5550e-01]],\n",
            "\n",
            "        [[ 1.2124e-02,  9.1738e-03,  2.8307e-01,  2.1436e-01,  3.8322e-02,\n",
            "           1.6121e-01,  1.4600e-01,  4.0203e-02],\n",
            "         [ 1.2860e-02,  1.0159e-02,  2.8353e-01,  2.1204e-01,  3.6764e-02,\n",
            "           1.5970e-01,  1.4690e-01,  3.9122e-02],\n",
            "         [ 2.3370e-04,  1.0734e-02,  2.7735e-01,  2.2325e-01,  3.2398e-02,\n",
            "           1.5060e-01,  1.4536e-01,  3.8573e-02],\n",
            "         [-1.0007e-03,  1.0541e-02,  2.7713e-01,  2.2460e-01,  3.1621e-02,\n",
            "           1.4955e-01,  1.4590e-01,  3.6826e-02]]],\n",
            "       grad_fn=<TransposeBackward0>)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-6-119487643.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_custom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-08\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-05\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Check if they are close enough.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Testing on data & compare\n",
        "output_custom = multi_head_attention(q, k, v, num_heads, d_model)\n",
        "print(output_custom)\n",
        "\n",
        "multihead_attn = torch.nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads, bias=False, batch_first=True)\n",
        "output, _ = multihead_attn(q, k, v)\n",
        "print(output)\n",
        "\n",
        "assert torch.allclose(output_custom, output, atol=1e-08, rtol=1e-05) # Check if they are close enough.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output.shape"
      ],
      "metadata": {
        "id": "ZmFkC1DXOKqj",
        "outputId": "0b849b3e-50c5-4c64-9876-7b3944099377",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 4, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_custom.shape"
      ],
      "metadata": {
        "id": "-3ngaFudMGHK",
        "outputId": "595a13d0-9634-4233-b237-4f916ff22444",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 4, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xTlugkHCOMUu"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}