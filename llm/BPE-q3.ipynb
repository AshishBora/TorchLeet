{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "081a9dd1",
   "metadata": {},
   "source": [
    "## Problem: Write a Byte Pain Encoder in Python\n",
    "\n",
    "### Problem Statement\n",
    "Implement a **Transformer model** in PyTorch by completing the required sections. The model should consist of an embedding layer, a Transformer encoder, and an output layer for sequence processing and prediction.\n",
    "\n",
    "### Requirements\n",
    "1. **Define the Transformer Model Architecture**:\n",
    "   - **Embedding Layer**:\n",
    "     - Implement a layer to transform input data into a higher-dimensional space.\n",
    "     - Use a `torch.nn.Linear` or `torch.nn.Embedding` layer to create embeddings from the input.\n",
    "   - **Transformer Encoder**:\n",
    "     - Use `torch.nn.TransformerEncoder` or `torch.nn.Transformer` to process sequences with attention.\n",
    "     - Configure parameters such as the number of attention heads and encoder layers.\n",
    "   - **Output Layer**:\n",
    "     - Add a fully connected (linear) layer to reduce the transformer's sequence output into the desired output dimension.\n",
    "\n",
    "2. **Implement the Forward Method**:\n",
    "   - Map the input to the higher-dimensional space using the embedding layer.\n",
    "   - Pass the transformed input through the Transformer encoder.\n",
    "   - Use the output layer to convert the encoded sequence into predictions.\n",
    "\n",
    "### Constraints\n",
    "- Handle input padding correctly for variable-length sequences.\n",
    "- Ensure compatibility with batch processing by correctly shaping input and output tensors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f09999",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "\n",
    "def get_vocab(corpus):\n",
    "    \"\"\"Creates a vocabulary with words split into characters and a special end-of-word token.\"\"\"\n",
    "    vocab = Counter()\n",
    "    for word in corpus:\n",
    "        tokens = list(word) + ['</w>']\n",
    "        vocab[tuple(tokens)] += 1\n",
    "    return vocab\n",
    "\n",
    "def get_stats(vocab):\n",
    "    \"\"\"Counts frequency of adjacent symbol pairs.\"\"\"\n",
    "    pairs = defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        for i in range(len(word) - 1):\n",
    "            pairs[(word[i], word[i + 1])] += freq\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, vocab):\n",
    "    \"\"\"Merges the most frequent pair into a single symbol.\"\"\"\n",
    "    new_vocab = {}\n",
    "    bigram = ' '.join(pair)\n",
    "    replacement = ''.join(pair)\n",
    "    for word, freq in vocab.items():\n",
    "        word_str = ' '.join(word)\n",
    "        # Replace bigram with merged symbol\n",
    "        new_word_str = word_str.replace(bigram, replacement)\n",
    "        new_vocab[tuple(new_word_str.split())] = freq\n",
    "    return new_vocab\n",
    "\n",
    "def byte_pair_encoding(corpus, num_merges=10):\n",
    "    \"\"\"Performs BPE on a corpus.\"\"\"\n",
    "    vocab = get_vocab(corpus)\n",
    "    merges = []\n",
    "    for _ in range(num_merges):\n",
    "        pairs = get_stats(vocab)\n",
    "        if not pairs:\n",
    "            break\n",
    "        best = max(pairs, key=pairs.get)\n",
    "        vocab = merge_vocab(best, vocab)\n",
    "        merges.append(best)\n",
    "        print(f\"Merge {_ + 1}: {best}\")\n",
    "    return vocab, merges\n",
    "\n",
    "# Example usage\n",
    "corpus = [\"low\", \"lowest\", \"newer\", \"wider\"]\n",
    "final_vocab, merge_operations = byte_pair_encoding(corpus, num_merges=10)\n",
    "\n",
    "print(\"\\nFinal Vocabulary:\")\n",
    "for word in final_vocab:\n",
    "    print(' '.join(word), \":\", final_vocab[word])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4638a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_get_vocab():\n",
    "    corpus = [\"test\"]\n",
    "    vocab = get_vocab(corpus)\n",
    "    assert vocab == {('t', 'e', 's', 't', '</w>'): 1}\n",
    "    print(\"✓ test_get_vocab passed\")\n",
    "\n",
    "def test_get_stats():\n",
    "    vocab = {('t', 'e', 's', 't', '</w>'): 1}\n",
    "    stats = get_stats(vocab)\n",
    "    expected = {\n",
    "        ('t', 'e'): 1,\n",
    "        ('e', 's'): 1,\n",
    "        ('s', 't'): 1,\n",
    "        ('t', '</w>'): 1\n",
    "    }\n",
    "    assert stats == expected\n",
    "    print(\"✓ test_get_stats passed\")\n",
    "\n",
    "def test_merge_vocab():\n",
    "    vocab = {('t', 'e', 's', 't', '</w>'): 1}\n",
    "    merged = merge_vocab(('e', 's'), vocab)\n",
    "    expected = {('t', 'es', 't', '</w>'): 1}\n",
    "    assert merged == expected\n",
    "    print(\"✓ test_merge_vocab passed\")\n",
    "\n",
    "def test_bpe_sequence():\n",
    "    corpus = [\"low\", \"lower\", \"newest\", \"widest\"]\n",
    "    final_vocab, merges = byte_pair_encoding(corpus, num_merges=5)\n",
    "    assert isinstance(final_vocab, dict)\n",
    "    assert all(isinstance(pair, tuple) for pair in merges)\n",
    "    assert len(merges) == 5\n",
    "    print(\"✓ test_bpe_sequence passed\")\n",
    "\n",
    "# Run all tests\n",
    "test_get_vocab()\n",
    "test_get_\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
